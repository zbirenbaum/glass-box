# Use the robust NVIDIA base
FROM nvcr.io/nvidia/pytorch:25.03-py3

# 1. Install 'uv' using the official multi-stage image
COPY --from=ghcr.io/astral-sh/uv:latest /uv /usr/local/bin/uv

# 2. Set Environment Variables
ENV CUDA_HOME=/usr/local/cuda \
    PATH="/usr/local/cuda/bin:/usr/local/bin:$PATH" \
    LC_ALL=en_US.UTF-8 \
    PIP_BREAK_SYSTEM_PACKAGES=1 \
    UV_PYTHON_INSTALL_DIR="/usr/local/share/uv/python" \
    UV_CACHE_DIR="/usr/local/share/uv/cache" \
    UV_COMPILE_BYTECODE=1 \
    UV_LINK_MODE=copy \
    NCCL_DEBUG=WARN \
    TORCH_NCCL_ASYNC_ERROR_HANDLING=1 \
    VLLM_ATTENTION_BACKEND=FLASH_ATTN \
    TORCH_CUDA_ARCH_LIST="9.0"

# 3. Install System Dependencies (Consolidated)
RUN apt-get update && apt-get install -y --no-install-recommends \
    build-essential \
    curl \
    git \
    git-lfs \
    net-tools \
    sudo \
    tmux \
    vim \
    && apt-get clean autoclean \
    && rm -rf /var/lib/apt/lists/* /tmp/* /var/tmp/* \
    && echo "LC_ALL=en_US.UTF-8" >> /etc/environment

# 4. Install Python Dependencies
WORKDIR /app

# Install packages via uv
# We use --system to install into the container's global environment
# We also add --break-system-packages explicitly for safety, though the ENV var usually handles it.
RUN uv pip install --system --break-system-packages --no-build-isolation \
    hatchling \
    "prime-rl[vllm] @ git+https://github.com/PrimeIntellect-ai/prime-rl.git" \
    verl \
    "ray[default]" \
    wandb \
    tensorboard \
    flash-attn --no-build-isolation \
    vllm
    # Install vLLM (pinned)
    # "vllm>=0.11.0"


# 5. Copy scripts
COPY train_grpo.py /app/train_grpo.py
