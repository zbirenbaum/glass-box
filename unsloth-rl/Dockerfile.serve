FROM nvidia/cuda:12.4.1-devel-ubuntu22.04

WORKDIR /app

# 1. Install System Dependencies
RUN apt-get update && apt-get install -y \
    python3 python3-pip git wget libgl1 libglib2.0-0 curl unzip && \
    ln -s /usr/bin/python3 /usr/bin/python

# 2. Install AWS CLI (for high-speed S3 sync)
RUN curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip" && \
    unzip awscliv2.zip && \
    ./aws/install && \
    rm -rf aws awscliv2.zip

# 3. Install Torch 2.4 (Required for FA3)
RUN pip install --upgrade pip && \
    pip install torch==2.4.0 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124

# 4. [FIX] Install Build Dependencies for Flash Attention
# 'packaging' caused your error. 'ninja' speeds up the build.
RUN pip install packaging ninja setuptools wheel

# 5. Install Flash Attention 3 (Beta for Hopper)
RUN git clone https://github.com/Dao-AILab/flash-attention.git && \
    cd flash-attention && \
    # Switch to valid FA3 commit if main is unstable, but main is usually fine
    export FLASH_ATTENTION_FORCE_BUILD=TRUE && \
    export MAX_JOBS=8 && \
    python setup.py install

# 6. Install vLLM
RUN pip install vllm>=0.6.0 accelerate transformers huggingface_hub

# 7. Copy the script
COPY serve.sh /app/serve.sh
RUN chmod +x /app/serve.sh

ENTRYPOINT ["/app/serve.sh"]
