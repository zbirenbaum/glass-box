apiVersion: apps/v1
kind: Deployment
metadata:
  name: qwen-inference-cluster
spec:
  replicas: 3 # <--- START WITH 2 NODES. SCALE UP TO 11 AS NEEDED.
  selector:
    matchLabels:
      app: qwen-serve
  template:
    metadata:
      labels:
        app: qwen-serve
    spec:
      imagePullSecrets:
      - name: regcred 
      containers:
      - name: vllm-engine
        image: zbirenbaum/qwen-serve-fa3:latest
        command: ["/bin/bash", "-c"]
        args:
        - |
          /app/serve.sh
        
        envFrom:
        - secretRef:
            name: qwen-env
        
        env:
        - name: AWS_REGION
          value: "US-WEST-04"
        - name: VLLM_ATTENTION_BACKEND
          value: "FLASH_ATTN_V3"
        - name: EXTRA_ARGS
          # TP=8 is crucial here to use all 8 GPUs on the node
          value: "--tensor-parallel-size 8 --enable-lora --lora-modules my-adapter=/data/lora_adapters/my-best-adapter"
          
        ports:
        - containerPort: 8000
        
        resources:
          limits:
            nvidia.com/gpu: 8 # <--- 1 REPLICA = 1 FULL NODE
            memory: "800Gi"
            cpu: "100"
          requests:
            nvidia.com/gpu: 8
            memory: "800Gi"
            cpu: "100"
            
        volumeMounts:
        - name: model-cache
          mountPath: /data/base_model
        - name: lora-storage
          mountPath: /data/lora_adapters
          
      volumes:
      - name: model-cache
        persistentVolumeClaim:
          claimName: qwen-weights-pvc
      - name: lora-storage
        persistentVolumeClaim:
          claimName: lora-weights-pvc
